{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Section 0: Import necessary libraries\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "!ls -l ./gdrive/MyDrive/Colab\\ Notebooks/datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pDvJHxd83fQW",
        "outputId": "3ceaa2c7-f6dc-4c94-a8fd-109427273a44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "total 164265\n",
            "-rw------- 1 root root     23875 Nov  6 01:33 diabetes2.csv\n",
            "-rw------- 1 root root     26657 Dec 15 03:41 submission.csv\n",
            "-rw------- 1 root root  33075636 Dec 14 05:43 test_data.csv\n",
            "-rw------- 1 root root 135047556 Dec 14 05:43 train_data.csv\n",
            "-rw------- 1 root root     32350 Dec 14 05:43 train_target.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Section 1: Load and preprocess the data\n",
        "\n",
        "# Load training data\n",
        "train_data = pd.read_csv('/content/gdrive/MyDrive/Colab Notebooks/datasets/train_data.csv', header=None)\n",
        "train_target = pd.read_csv('/content/gdrive/MyDrive/Colab Notebooks/datasets/train_target.csv', header=None)\n",
        "\n",
        "# Load testing data\n",
        "test_data = pd.read_csv('/content/gdrive/MyDrive/Colab Notebooks/datasets/test_data.csv', header=None)\n",
        "\n",
        "# Define a custom dataset class\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, data, target, transform=None):\n",
        "        self.data = data\n",
        "        self.target = target\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = self.data.iloc[idx, :].values.astype(np.uint8).reshape(48, 48)\n",
        "        image = Image.fromarray(image)\n",
        "        target = self.target.iloc[idx, 0]\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, target\n",
        "\n",
        "# Define transformations for the data\n",
        "transform = transforms.Compose([\n",
        "    transforms.Grayscale(num_output_channels=1),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "# Create datasets and dataloaders\n",
        "X_train, X_val, y_train, y_val = train_test_split(train_data, train_target, test_size=0.2, random_state=42)\n",
        "\n",
        "train_dataset = CustomDataset(X_train, y_train, transform=transform)\n",
        "val_dataset = CustomDataset(X_val, y_val, transform=transform)\n",
        "test_dataset = CustomDataset(test_data, pd.DataFrame(np.zeros((len(test_data), 1))), transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
      ],
      "metadata": {
        "id": "tT6zH1s53lFo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Section 2: Build the neural network model\n",
        "\n",
        "class CNNModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNNModel, self).__init__()\n",
        "\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        )\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(256 * 3 * 3, 512),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(p=0.4),\n",
        "\n",
        "            nn.Linear(512, 256),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(p=0.4),\n",
        "\n",
        "            nn.Linear(256, 3)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "# Instantiate the improved model and define loss function and optimizer\n",
        "model = CNNModel()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "rc545zWT3rPx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Section 3: Train the model\n",
        "\n",
        "num_epochs = 15\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    for inputs, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "# Section 4: Evaluate the model on the validation set\n",
        "    model.eval()\n",
        "    val_predictions = []\n",
        "    val_targets = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in val_loader:\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            val_predictions.extend(predicted.cpu().numpy())\n",
        "            val_targets.extend(labels.cpu().numpy())\n",
        "\n",
        "    val_accuracy = accuracy_score(val_targets, val_predictions)\n",
        "    print(f'Validation Accuracy: {val_accuracy}')"
      ],
      "metadata": {
        "id": "UQuGYdSd3xEI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11a1e477-385d-435b-8688-299bb7c73564"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.7020092735703246\n",
            "Validation Accuracy: 0.7394126738794435\n",
            "Validation Accuracy: 0.7462132921174652\n",
            "Validation Accuracy: 0.6772797527047913\n",
            "Validation Accuracy: 0.7489953632148377\n",
            "Validation Accuracy: 0.7604327666151468\n",
            "Validation Accuracy: 0.7486862442040185\n",
            "Validation Accuracy: 0.7394126738794435\n",
            "Validation Accuracy: 0.7474497681607419\n",
            "Validation Accuracy: 0.740030911901082\n",
            "Validation Accuracy: 0.7261205564142195\n",
            "Validation Accuracy: 0.7493044822256569\n",
            "Validation Accuracy: 0.7582689335394127\n",
            "Validation Accuracy: 0.7598145285935085\n",
            "Validation Accuracy: 0.768160741885626\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Section 5: Make predictions on the test set\n",
        "\n",
        "model.eval()\n",
        "test_predictions = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, _ in test_loader:\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        test_predictions.extend(predicted.cpu().numpy())"
      ],
      "metadata": {
        "id": "KXwrxiHX346Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Section 6: Save predictions to a CSV file\n",
        "\n",
        "submission_df = pd.DataFrame({'Id': range(len(test_predictions)), 'Category': test_predictions})\n",
        "submission_df.to_csv('/content/gdrive/MyDrive/Colab Notebooks/datasets/submission.csv', index=False)"
      ],
      "metadata": {
        "id": "VBQxrjQD37se"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Section 7: Print the final accuracy score\n",
        "\n",
        "print(f'Final Validation Accuracy: {val_accuracy}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "onzqsQhY3_NC",
        "outputId": "ac65cbf4-bf1e-48e8-f878-e37eacc023a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Validation Accuracy: 0.768160741885626\n"
          ]
        }
      ]
    }
  ]
}